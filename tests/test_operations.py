"""Tests for different operations.

Note that in each test, we have to reimport all modules involving keras, such
that when the backend is switched, the functions inside are reimported with the
correct backend.
"""

import math

import keras
import numpy as np
import pytest
from scipy.ndimage import gaussian_filter
from scipy.signal import hilbert as hilbert_scipy

from zea import ops
from zea.func.ultrasound import (
    channels_to_complex,
    complex_to_channels,
    compute_time_to_peak_stack,
    make_tgc_curve,
)
from zea.ops import Pipeline, Simulate
from zea.probes import Probe
from zea.scan import Scan

from . import DEFAULT_TEST_SEED, backend_equality_check


@pytest.mark.parametrize(
    "comp_type, size, parameter_value_range",
    [
        ("a", (2, 1, 128, 32), (50, 200)),
        ("a", (512, 512), (50, 200)),
        ("mu", (2, 1, 128, 32), (50, 300)),
        ("mu", (512, 512), (50, 300)),
    ],
)
@backend_equality_check(decimal=4)
def test_companding(comp_type, size, parameter_value_range):
    """Test companding function"""

    import keras

    from zea import ops

    rng = np.random.default_rng(42)

    for parameter_value in np.linspace(*parameter_value_range, 10):
        A = parameter_value if comp_type == "a" else 0
        mu = parameter_value if comp_type == "mu" else 0

        companding = ops.Companding(comp_type=comp_type, expand=False)
        rng = np.random.default_rng(DEFAULT_TEST_SEED)
        signal = np.clip((rng.standard_normal(size) - 0.5) * 2, -1, 1)
        signal = signal.astype("float32")
        signal = keras.ops.convert_to_tensor(signal)

        signal_out = companding(data=signal, A=A, mu=mu)["data"]

        signal = keras.ops.convert_to_numpy(signal)
        signal_out = keras.ops.convert_to_numpy(signal_out)

        assert np.any(np.not_equal(signal, signal_out)), (
            "Companding failed, arrays should not be equal"
        )

        companding = ops.Companding(comp_type=comp_type, expand=True)

        signal_out = keras.ops.convert_to_tensor(signal_out)
        signal_out = companding(data=signal_out, A=A, mu=mu)["data"]

        signal = keras.ops.convert_to_numpy(signal)
        signal_out = keras.ops.convert_to_numpy(signal_out)

        np.testing.assert_almost_equal(signal, signal_out, decimal=6)
    return signal_out


@pytest.mark.parametrize(
    "size, dynamic_range, input_range",
    [
        ((2, 1, 128, 32), (-30, -5), None),
        ((512, 512), None, (0, 1)),
        ((512, 600), None, (-10, 300)),
        ((1, 128, 32), None, None),
    ],
)
@backend_equality_check(decimal=4)
def test_converting_to_image(size, dynamic_range, input_range):
    """Test converting to image functions"""

    import keras

    from zea import ops

    if dynamic_range is None:
        _dynamic_range = (-60, 0)
    else:
        _dynamic_range = dynamic_range
    if input_range is None:
        _input_range = (0, 1)
    else:
        _input_range = input_range

    rng = np.random.default_rng(DEFAULT_TEST_SEED)
    data = rng.standard_normal(size) * (_input_range[1] - _input_range[0]) + _input_range[0]
    output_range = (0, 1)
    normalize = ops.Normalize(output_range, input_range)
    log_compress = ops.LogCompress()

    data = keras.ops.convert_to_tensor(data)

    _data = normalize(data=data)["data"]
    _data = log_compress(data=_data, dynamic_range=dynamic_range)["data"]

    _data = keras.ops.convert_to_numpy(_data)

    # data should be in dynamic range
    assert np.all(
        np.logical_and(_data >= _dynamic_range[0], _data <= _dynamic_range[1]),
    ), f"Data is not in dynamic range after converting to image {_dynamic_range}"
    return _data


@pytest.mark.parametrize(
    "size, output_range, input_range",
    [
        ((2, 1, 128, 32), (-30, -5), (0, 1)),
        ((512, 512), (-2, -1), (-3, 50)),
        ((1, 128, 32), (50, 51), (-2.2, 3.0)),
    ],
)
@backend_equality_check(decimal=4)
def test_normalize(size, output_range, input_range):
    """Test normalize function"""

    import keras

    from zea import ops

    normalize = ops.Normalize(output_range, input_range)

    _input_range = output_range
    _output_range = input_range
    normalize_back = ops.Normalize(_output_range, _input_range)

    # create random data between input range
    rng = np.random.default_rng(DEFAULT_TEST_SEED)
    data = rng.random(size) * (input_range[1] - input_range[0]) + input_range[0]

    data = keras.ops.convert_to_tensor(data)

    _data = normalize(data=data)["data"]

    input_range, output_range = output_range, input_range
    _data = normalize_back(data=_data)["data"]

    _data = keras.ops.convert_to_numpy(_data)
    data = keras.ops.convert_to_numpy(data)

    np.testing.assert_almost_equal(data, _data, decimal=4)
    return _data


@pytest.mark.parametrize(
    "size, axis",
    [
        ((2, 1, 128, 32), (-1)),
        ((512, 512), (-1)),
        ((1, 128, 32), (-1)),
    ],
)
def test_complex_to_channels(size, axis):
    """Test complex to channels and back"""
    rng = np.random.default_rng(DEFAULT_TEST_SEED)
    data = rng.random(size) + 1j * rng.random(size)
    _data = complex_to_channels(data, axis=axis)
    __data = channels_to_complex(_data)
    np.testing.assert_almost_equal(data, __data)


@pytest.mark.parametrize(
    "size, axis",
    [
        ((222, 1, 2, 2), (-1)),
        ((512, 512, 2), (-1)),
        ((2, 20, 128, 2), (-1)),
    ],
)
def test_channels_to_complex(size, axis):
    """Test channels to complex and back"""
    rng = np.random.default_rng(DEFAULT_TEST_SEED)
    data = rng.random(size)
    _data = channels_to_complex(data)
    __data = complex_to_channels(_data, axis=axis)
    np.testing.assert_almost_equal(data, __data)


@pytest.mark.parametrize(
    "factor, batch_size",
    [
        (1, 2),
        (4, 1),
        (2, 3),
    ],
)
def test_up_and_down_conversion(factor, batch_size):
    """Test rf2iq and iq2rf in sequence"""
    n_el = 128
    n_scat = 3
    n_tx = 2
    n_ax = 512

    aperture = 30e-3

    tx_apodizations = np.ones((n_tx, n_el))
    probe_geometry = np.stack(
        [
            np.linspace(-aperture / 2, aperture / 2, n_el),
            np.zeros(n_el),
            np.zeros(n_el),
        ],
        axis=1,
    )

    t0_delays = np.stack(
        [
            np.linspace(0, 1e-6, n_el),
            np.linspace(1e-6, 0, n_el),
        ]
    )

    scan = Scan(
        n_tx=n_tx,
        n_ax=n_ax,
        n_el=n_el,
        center_frequency=3.125e6,
        sampling_frequency=12.5e6,
        probe_geometry=probe_geometry,
        t0_delays=t0_delays,
        tx_apodizations=tx_apodizations,
        element_width=np.linalg.norm(probe_geometry[1] - probe_geometry[0]),
        apply_lens_correction=True,
        lens_sound_speed=1440.0,
        lens_thickness=1e-3,
        initial_times=np.zeros((n_tx,)),
        attenuation_coef=0.7,
        n_ch=1,
        selected_transmits="all",
    )
    probe = Probe(
        probe_geometry=probe_geometry,
        center_frequency=3.125e6,
        sampling_frequency=12.5e6,
    )

    # use pipeline here so it is easy to propagate the scan parameters
    simulator_pipeline = Pipeline(
        [
            Simulate(output_key="simulated_data"),
            ops.Demodulate(key="simulated_data", output_key="data"),
            ops.Downsample(factor=factor),
            ops.UpMix(upsampling_rate=factor),
        ]
    )
    parameters = simulator_pipeline.prepare_parameters(probe=probe, scan=scan)

    data = []
    _data = []
    rng = np.random.default_rng(DEFAULT_TEST_SEED)
    for _ in range(batch_size):
        # Define scatterers with random variation
        scat_x_base, scat_z_base = np.meshgrid(
            np.linspace(-10e-3, 10e-3, 5),
            np.linspace(5e-3, 30e-3, 5),
            indexing="ij",
        )
        # Add random perturbations
        scat_x = np.ravel(scat_x_base) + rng.uniform(-1e-3, 1e-3, 25)
        scat_z = np.ravel(scat_z_base) + rng.uniform(-1e-3, 1e-3, 25)
        n_scat = len(scat_x)
        # Select random subset of scatterers
        idx = rng.choice(n_scat, n_scat, replace=False)[:n_scat]
        scat_positions = np.stack(
            [
                scat_x[idx],
                np.zeros_like(scat_x[idx]),
                scat_z[idx],
            ],
            axis=1,
        )
        scat_positions = np.expand_dims(scat_positions, axis=0)  # add batch dimension

        output = simulator_pipeline(
            **parameters,
            scatterer_positions=scat_positions.astype(np.float32),
            scatterer_magnitudes=np.ones((1, n_scat), dtype=np.float32),
        )

        data.append(keras.ops.convert_to_numpy(output["data"]))
        _data.append(keras.ops.convert_to_numpy(output["simulated_data"]))

    data = np.concatenate(data)
    _data = np.concatenate(_data)

    np.testing.assert_almost_equal(
        data,
        _data,
        decimal=2,
        err_msg="Data is not equal after up and down conversion.",
    )


@pytest.mark.parametrize(
    "shape, axis, N",
    [
        # 1D tests
        ((500,), -1, None),
        ((500,), -1, 500),
        ((500,), -1, 1024),
        # 2D tests
        ((128, 500), -1, None),
        ((128, 500), -1, 512),
        # 3D tests
        ((2, 500, 128), 1, None),
        ((2, 500, 128), 1, 600),
        # 4D tests (original test case)
        ((2, 500, 128, 1), -3, None),
        ((2, 500, 128, 1), -3, 512),
    ],
)
@backend_equality_check(decimal=4)
def test_hilbert_transform(shape, axis, N):
    """Test hilbert transform with various shapes and N values"""

    import keras

    from zea import func

    rng = np.random.default_rng(DEFAULT_TEST_SEED)

    # Create sinusoidal data along the specified axis
    n_samples = shape[axis]
    base_signal = np.sin(np.linspace(0, 2 * math.e * np.pi, n_samples))

    # Reshape to match target shape
    reshape_spec = [1] * len(shape)
    reshape_spec[axis] = n_samples
    data = base_signal.reshape(reshape_spec)

    # Tile to full shape
    tile_spec = list(shape)
    tile_spec[axis] = 1
    data = np.tile(data, tile_spec)

    # Add small noise
    data = data + rng.random(data.shape) * 0.1

    data = keras.ops.convert_to_tensor(data)

    data_iq = func.hilbert(data, N=N, axis=axis)
    assert keras.ops.dtype(data_iq) in [
        "complex64",
        "complex128",
    ], f"Data type should be complex, got {keras.ops.dtype(data_iq)} instead."

    data_iq = keras.ops.convert_to_numpy(data_iq)

    reference_data_iq = hilbert_scipy(data, N=N, axis=axis)
    np.testing.assert_almost_equal(reference_data_iq, data_iq, decimal=4)

    return data_iq


def test_hilbert_transform_invalid_N():
    """Test that hilbert raises ValueError when N < n_ax"""
    import keras

    from zea import func

    rng = np.random.default_rng(DEFAULT_TEST_SEED)
    data = rng.random((100,))
    data = keras.ops.convert_to_tensor(data)

    # N=50 is less than n_ax=100, should raise ValueError
    with pytest.raises(ValueError, match="N must be greater or equal to n_ax"):
        func.hilbert(data, N=50, axis=-1)


@pytest.fixture(scope="module")
def spiral_image():
    """
    Fixture for generating a synthetic spiral image and noisy variants.
    Returns:
        dict: {
            "spiral": clean spiral image of size (64, 64),
            "noisy": additive Gaussian noise,
            "speckle": multiplicative speckle noise
        }
    """
    x = np.linspace(-1, 1, 64)
    y = np.linspace(-1, 1, 64)
    xv, yv = np.meshgrid(x, y)
    r = np.sqrt(xv**2 + yv**2)
    theta = np.arctan2(yv, xv)
    spiral = np.sin(8 * theta + 8 * r)
    spiral = (spiral - spiral.min()) / (spiral.max() - spiral.min())

    rng = np.random.default_rng(DEFAULT_TEST_SEED)
    noisy = spiral + 0.2 * rng.normal(size=spiral.shape)
    noisy = np.clip(noisy, 0, 1)
    speckle = spiral * (1 + 0.5 * rng.normal(size=spiral.shape))
    speckle = np.clip(speckle, 0, 1)

    return {
        "spiral": spiral.astype(np.float32),
        "noisy": noisy.astype(np.float32),
        "speckle": speckle.astype(np.float32),
    }


@pytest.mark.parametrize("sigma", [0.5, 1.0, 2.0])
@backend_equality_check(decimal=4, backends=["tensorflow", "jax"])
def test_gaussian_blur(sigma, spiral_image):
    """
    Test `ops.GaussianBlur against scipy.ndimage.gaussian_filter.`
    `GaussianBlur` with default args should be equivalent to scipy.

    NOTE: We don't test torch backend here because of differences in padding behavior.
    """
    import keras

    from zea import ops

    blur = ops.GaussianBlur(sigma=sigma, with_batch_dim=False)

    # Use spiral image for testing
    image = spiral_image["spiral"]
    image_tensor = keras.ops.convert_to_tensor(image[..., None])

    blurred_scipy = gaussian_filter(image, sigma=sigma)
    blurred_zea = blur(data=image_tensor)["data"][..., 0]

    blurred_zea = keras.ops.convert_to_numpy(blurred_zea)

    np.testing.assert_allclose(blurred_scipy, blurred_zea, rtol=1e-5, atol=1e-5)

    return blurred_zea


@pytest.mark.parametrize("sigma", [1.0, 2.0])
@backend_equality_check(decimal=5, backends=["tensorflow", "jax"])
def test_lee_filter(sigma, spiral_image):
    """
    Test `ops.LeeFilter`, checks if variance is reduced and if with and without
    batch dimension give the same result.

    # NOTE: We don't test torch backend here because of differences in padding behavior.
    """
    import keras

    from zea import ops

    # Use spiral image for testing
    image = spiral_image["spiral"]
    image = image[..., None]  # add channel dimension

    lee = ops.LeeFilter(sigma=sigma, with_batch_dim=False)
    lee_batched = ops.LeeFilter(sigma=sigma, with_batch_dim=True)

    image_tensor = keras.ops.convert_to_tensor(image)
    filtered = lee(data=image_tensor)["data"][..., 0]
    filtered_batched = lee_batched(data=image_tensor[None, ...])["data"][0, ..., 0]

    filtered = keras.ops.convert_to_numpy(filtered)
    filtered_batched = keras.ops.convert_to_numpy(filtered_batched)

    assert np.allclose(filtered, filtered_batched), (
        "LeeFilter with and without batch dim should give the same result."
    )

    assert np.var(filtered) < np.var(image), (
        "LeeFilter should reduce variance of the processed image"
    )

    return filtered


@pytest.mark.parametrize(
    "threshold_type,below_threshold,fill_value,threshold_param",
    [
        ("hard", True, "min", {"percentile": 50}),
        ("hard", False, "max", {"percentile": 75}),
        ("soft", True, 0.0, {"threshold": 0.5}),
        ("soft", False, 1.0, {"threshold": 0.3}),
    ],
)
@backend_equality_check()
def test_threshold_op(spiral_image, threshold_type, below_threshold, fill_value, threshold_param):
    """Test `ops.Threshold` operation on a synthetic spiral image."""
    import keras

    from zea import ops

    spiral = spiral_image["spiral"]
    spiral_tensor = keras.ops.convert_to_tensor(spiral)

    threshold = ops.Threshold(
        threshold_type=threshold_type,
        below_threshold=below_threshold,
        fill_value=fill_value,
    )

    # Set the correct parameter (either percentile or threshold) and the other to None
    percentile = threshold_param.get("percentile", None)
    threshold_value = threshold_param.get("threshold", None)

    out = threshold(data=spiral_tensor, percentile=percentile, threshold=threshold_value)
    out_np = keras.ops.convert_to_numpy(out["data"])

    # Quantitative: check that thresholding changes the image
    assert not np.allclose(spiral, out_np)

    return out_np


@pytest.mark.parametrize(
    "niter,lmbda",
    [
        (5, 0.5),
        (10, 0.25),
    ],
)
@backend_equality_check()
def test_anisotropic_diffusion_op(spiral_image, niter, lmbda):
    """Test `ops.AnisotropicDiffusion` operation on a noisy synthetic image."""

    import keras

    from zea import ops

    speckle = spiral_image["speckle"]
    speckle_tensor = keras.ops.convert_to_tensor(speckle)

    srad = ops.AnisotropicDiffusion(with_batch_dim=False)
    filtered = srad(data=speckle_tensor, niter=niter, lmbda=lmbda)
    filtered_np = keras.ops.convert_to_numpy(filtered["data"])

    # Quantitative: variance should be reduced, but mean should be similar
    assert np.var(filtered_np) < np.var(speckle)
    assert np.abs(np.mean(filtered_np) - np.mean(speckle)) < 0.1

    return filtered_np


def test_compute_time_to_peak():
    """Test compute_time_to_peak function."""
    t = np.arange(512) / 250e6

    t_peak = 1e-6

    center_frequency0 = 5e6
    waveform0 = np.exp(-((t - t_peak) ** 2) / (2 * (0.2e-6) ** 2)) * np.cos(
        2 * np.pi * center_frequency0 * (t - t_peak)
    )

    center_frequency1 = 10e6
    waveform1 = np.exp(-((t - t_peak) ** 2) / (2 * (0.2e-6) ** 2)) * np.cos(
        2 * np.pi * center_frequency1 * (t - t_peak)
    )

    center_frequencies = np.array([center_frequency0, center_frequency1])
    waveforms = np.array([waveform0, waveform1])

    t_peak = compute_time_to_peak_stack(waveforms, center_frequencies, 250e6)

    assert np.allclose(t_peak, 1e-6, atol=1e-8), f"t_peak should be close to 1e-6, got {t_peak}"


@pytest.mark.parametrize(
    "beamformer,expected_shape",
    [
        ("das", (1, 7, 2)),
        ("dmas", (1, 7, 2)),
    ],
)
def test_beamformers(beamformer, expected_shape):
    """Test all beamformer operations (DAS and DMAS stages)."""
    if beamformer == "das":
        operation = ops.DelayAndSum(with_batch_dim=True)
    elif beamformer == "dmas":
        operation = ops.DelayMultiplyAndSum(with_batch_dim=True)
    else:
        raise ValueError(f"Unknown beamformer: {beamformer}")

    data = keras.ops.zeros((1, 3, 7, 4, 2))

    output = operation(data=data)["data"]
    assert output.shape == expected_shape, (
        f"Output shape should be {expected_shape} for {beamformer}, got {output.shape}"
    )

    # DMAS should raise ValueError with single channel
    if beamformer == "dmas":
        with pytest.raises(ValueError):
            data = keras.ops.zeros((1, 3, 7, 4, 1))
            operation(data=data)


@pytest.mark.parametrize(
    "axis, size, start, end, window_type",
    [
        (0, 64, 64, 32, "hanning"),
        (1, 32, 32, 16, "linear"),
    ],
)
def test_apply_window(axis, size, start, end, window_type):
    """Test ApplyWindow operation."""
    operation = ops.ultrasound.ApplyWindow(
        axis=axis, size=size, start=start, end=end, window_type=window_type
    )

    data = keras.ops.ones((256, 128, 64))
    data_out = operation(data=data)["data"]
    if axis == 0:
        assert data_out[:start, :, :].numpy().sum() == 0.0, "Start region not zeroed correctly."
        assert data_out[-end:, :, :].numpy().sum() == 0.0, "End region not zeroed correctly."
    elif axis == 1:
        assert data_out[:, :start, :].numpy().sum() == 0.0, "Start region not zeroed correctly."
        assert data_out[:, -end:, :].numpy().sum() == 0.0, "End region not zeroed correctly."


def test_make_tgc_curve():
    """Test that TGC curve is monotonically increasing with depth."""
    n_ax = 1000
    attenuation_coef = 0.5  # dB/cm/MHz (typical for soft tissue)
    sampling_frequency = 40e6  # 40 MHz
    center_frequency = 5e6  # 5 MHz
    sound_speed = 1540  # m/s

    tgc_curve = make_tgc_curve(
        n_ax=n_ax,
        attenuation_coef=attenuation_coef,
        sampling_frequency=sampling_frequency,
        center_frequency=center_frequency,
        sound_speed=sound_speed,
    )

    # Check output shape
    assert tgc_curve.shape == (n_ax,), f"Expected shape ({n_ax},), got {tgc_curve.shape}"

    # Check output dtype
    assert tgc_curve.dtype == np.float32, f"Expected dtype float32, got {tgc_curve.dtype}"

    # Check that the curve is monotonically increasing (TGC should increase with depth)
    differences = np.diff(tgc_curve)
    assert np.all(differences >= 0), "TGC curve should be monotonically increasing with depth"

    # Check that the first value is 1 (no gain at zero depth)
    assert np.isclose(tgc_curve[0], 1.0, rtol=1e-5), (
        f"TGC curve should start at 1.0 (no gain at zero depth), got {tgc_curve[0]}"
    )

    # Check that values are positive
    assert np.all(tgc_curve > 0), "TGC curve values should be positive"


@pytest.mark.parametrize(
    "n_tx, n_pix, n_rx, with_batch",
    [
        (32, 50, 32, False),  # Square aperture, no batch
        (32, 50, 32, True),  # Square aperture, with batch
        (48, 100, 48, False),  # Larger aperture, no batch
        (20, 25, 20, False),  # Smaller aperture, no batch
    ],
)
@backend_equality_check(decimal=5)
def test_common_midpoint_phase_error(n_tx, n_pix, n_rx, with_batch):
    """Test CommonMidpointPhaseError operation.

    This operation computes the Common Midpoint Phase Error (CMPE) between
    translated transmit and receive subapertures, used for autofocusing.
    """
    from zea import ops

    # Create synthetic TOF-corrected IQ data
    rng = np.random.default_rng(DEFAULT_TEST_SEED)

    # Create data with some coherent structure (simulating actual TOF-corrected data)
    # Add random phase variations to simulate realistic ultrasound data
    # IQ data has 2 channels (I and Q)
    phase = rng.random((n_tx, n_pix, n_rx)) * 2 * np.pi
    amplitude = rng.random((n_tx, n_pix, n_rx)) * 0.5 + 0.5  # Amplitude between 0.5 and 1

    # Convert to IQ format
    data = np.stack(
        [
            amplitude * np.cos(phase),  # I channel
            amplitude * np.sin(phase),  # Q channel
        ],
        axis=-1,
    ).astype(np.float32)

    # Test with batch dimension if requested
    if with_batch:
        batch_size = 2
        data = np.stack([data, data * 0.8], axis=0)  # Create two slightly different samples
        cmpe = ops.CommonMidpointPhaseError(with_batch_dim=True)
        expected_output_shape = (batch_size, n_pix)
    else:
        cmpe = ops.CommonMidpointPhaseError(with_batch_dim=False)
        expected_output_shape = (n_pix,)

    data_tensor = keras.ops.convert_to_tensor(data)

    # Compute CMPE
    output = cmpe(data=data_tensor)
    phase_error = output["data"]

    # Convert to numpy for assertions
    phase_error_np = keras.ops.convert_to_numpy(phase_error)

    # Check output shape
    assert phase_error_np.shape == expected_output_shape, (
        f"Expected shape {expected_output_shape}, got {phase_error_np.shape}"
    )

    # Check that output values are in valid range [0, π]
    # Phase differences should be absolute values bounded by π
    assert np.all(phase_error_np >= 0), "Phase errors should be non-negative"
    assert np.all(phase_error_np <= np.pi + 1e-5), (
        f"Phase errors should be <= π, got max: {np.max(phase_error_np)}"
    )

    # Check that not all values are zero (would indicate wrong computation)
    assert np.any(phase_error_np > 0), (
        "Phase errors should have some non-zero values for random data"
    )

    # Check for NaN values
    assert not np.any(np.isnan(phase_error_np)), "Phase errors should not contain NaN values"

    return phase_error_np


def test_common_midpoint_phase_error_with_zeros():
    """Test CommonMidpointPhaseError operation handles zeros correctly.

    The operation should handle cases where some data points are zero
    (e.g., points outside the field of view).
    """
    # Create small synthetic data
    n_tx, n_pix, n_rx = 24, 30, 24

    rng = np.random.default_rng(DEFAULT_TEST_SEED)

    # Create data with some zeros
    # IQ data has 2 channels (I and Q)
    phase = rng.random((n_tx, n_pix, n_rx)) * 2 * np.pi
    amplitude = rng.random((n_tx, n_pix, n_rx)) * 0.5 + 0.5

    data = np.stack(
        [
            amplitude * np.cos(phase),
            amplitude * np.sin(phase),
        ],
        axis=-1,
    ).astype(np.float32)

    # Set some regions to zero (simulating out-of-FOV points)
    data[:5, :, :5, :] = 0  # Zero out corner region
    data[-5:, :, -5:, :] = 0  # Zero out another corner

    cmpe = ops.CommonMidpointPhaseError(with_batch_dim=False)
    data_tensor = keras.ops.convert_to_tensor(data)

    output = cmpe(data=data_tensor)
    phase_error = keras.ops.convert_to_numpy(output["data"])

    # Check that computation doesn't crash and produces valid output
    assert phase_error.shape == (n_pix,), f"Expected shape ({n_pix},), got {phase_error.shape}"

    # Check for NaN or inf values
    assert np.all(np.isfinite(phase_error)), "Phase errors should be finite even with zeros in data"

    # Check that values are in valid range
    assert np.all(phase_error >= 0) and np.all(phase_error <= np.pi + 1e-5), (
        "Phase errors should be in [0, π] range"
    )


def test_common_midpoint_phase_error_coherent_data():
    """Test CommonMidpointPhaseError with perfectly coherent data.

    With perfectly coherent data (same phase everywhere), the phase error
    should be close to zero.
    """
    n_tx, n_pix, n_rx, n_ch = 32, 40, 32, 2

    # Create perfectly coherent data (same phase for all elements)
    amplitude = 1.0
    phase = np.pi / 4  # Single phase value

    data = np.full((n_tx, n_pix, n_rx, n_ch), 0.0, dtype=np.float32)
    data[..., 0] = amplitude * np.cos(phase)  # I channel
    data[..., 1] = amplitude * np.sin(phase)  # Q channel

    cmpe = ops.CommonMidpointPhaseError(with_batch_dim=False)
    data_tensor = keras.ops.convert_to_tensor(data)

    output = cmpe(data=data_tensor)
    phase_error = keras.ops.convert_to_numpy(output["data"])

    # For perfectly coherent data, phase errors should be very small
    assert np.all(phase_error < 0.01), (
        f"Phase errors for coherent data should be near zero, got max: {np.max(phase_error)}"
    )

    # Check shape
    assert phase_error.shape == (n_pix,), f"Expected shape ({n_pix},), got {phase_error.shape}"
